---
created: 2026-01-23
tags:
  - type/discipline
  - status/evergreen
  - topic/cognitive-science
  - topic/cognitive-neuroscience
  - topic/systems
  - topic/information-processing-theory
publish: true
---
# ðŸ§  Information Theory

### ðŸ§ Definition (The Scope)
>The mathematical study of the quantification, storage, and communication of information. Originally developed by **Claude Shannon** in 1948 to optimize telephone lines, it was rapidly adopted by psychologists to solve the "Black Box" problem of Behaviorism. In Cognitive Science, it provides the fundamental unit of measurementâ€”the **Bit** (binary digit)â€”to describe mental processes. It treats the brain not as a mystical spirit, but as an **Information Processing System** subject to physical limits of bandwidth, noise, and channel capacity.

---

### ðŸ”‘ Core Concepts (The Bricks)
- **[[The Bit]] (Uncertainty Reduction):** Information is defined not by "meaning," but by the reduction of uncertainty. One bit is the amount of information required to choose between two equally probable alternatives (e.g., a coin flip). If the brain eliminates half the possibilities, it has processed 1 bit.
    
- **[[Channel Capacity]] (Bandwidth):** The maximum rate at which information can be transmitted over a communication channel with a small error rate. In the human mind, this manifests as **Attention**. We have a finite "throughput"â€”we cannot process infinite stimuli simultaneously.
    
- **[[Entropy]] (Information):** In this context, Entropy ($H$) is a measure of the unpredictability of a state. High entropy means high uncertainty (maximum information potential); low entropy means high predictability (redundancy). The brain acts as a "prediction machine" aimed at minimizing the entropy of sensory input.
    
- **[[Signal-to-Noise Ratio]]:** The ratio of useful information (Signal) to irrelevant interference (Noise). The central task of perception (e.g., the "Cocktail Party Effect") is filtering the noise to isolate the signal.
### ðŸ“š Foundational Texts
- **[[A Mathematical Theory of Communication]]** by **Claude Shannon** (The paper that started it all).
    
- **[[The Magical Number Seven, Plus or Minus Two]]** by **George Miller** (The paper that applied Shannonâ€™s "Channel Capacity" to human memory).
    
- **[[Cybernetics]]** by **Norbert Wiener** (The parallel development of control and communication in animals).
    
- **[[Spikes: Exploring the Neural Code]]** by **Rieke et al.** (Applying information theory to neural spike trains).

### ðŸ§ª Unresolved Questions
* The Semantic Problem: Shannon explicitly stated that "semantic aspects of communication are irrelevant to the engineering problem." Yet, the mind cares _only_ about meaning. How do we bridge the gap between "Bits" (syntax) and "Meaning" (semantics)?
- Neural Coding: Does the brain encode information in the **Rate** of firing (Rate Coding) or the specific **Timing** of the spikes (Temporal Coding)?